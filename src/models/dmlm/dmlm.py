# Copyright (c) 2025
# SPDX-License-Identifier: Apache-2.0
#
# DMLM: Diffusion Material Language Model
# è¯¥æ–‡ä»¶åŸºäºåŸå§‹ DPLM (DiffusionProteinLanguageModel) å®Œæ•´æ¶æ„æ”¹å†™ï¼Œ
# ä¿ç•™ä¸­é—´ backbone ä¸ç”Ÿæˆ/è§£ç é€»è¾‘ï¼ŒåªæŠŠ tokenizer æ›¿æ¢ä¸º CrystaTokenizerWrapperï¼Œ
# å¹¶ç»™å‡º embedding å¯¹é½çš„é’©å­ï¼Œç”¨äºä»å¤´è®­ç»ƒï¼ˆä¸åŠ è½½ dplm æƒé‡ï¼‰ã€‚
#
# æ³¨æ„ï¼š
# - è¯·ç¡®ä¿ CrystaTokenizerWrapper çš„å®ç°å­˜åœ¨å¹¶å¯ importï¼ˆä¸‹æ–¹å°è¯•äº†å‡ ç§å¯¼å…¥æ–¹å¼ï¼‰ã€‚
# - è‹¥ get_net(...) è¿”å›çš„ net ä¸­å†…ç½®äº† embeddingï¼Œéœ€è¦åœ¨åˆå§‹åŒ–æ—¶æ ¹æ® crysta vocab_size é‡æ–°åˆå§‹åŒ– embeddingã€‚
# - æœ¬æ–‡ä»¶å°½é‡ä¸åŸ dplm.py ä¿æŒä¸€è‡´ï¼ˆå‡½æ•°å/æ¥å£/è¡Œä¸ºï¼‰ï¼Œä»¥ä¾¿ä¸è®­ç»ƒ pipeline æ— ç¼è¡”æ¥ã€‚

import math
import os
from dataclasses import dataclass, field

import numpy as np
import torch
import torch.nn as nn
from omegaconf import OmegaConf
from tqdm import tqdm
from transformers import AutoConfig, AutoTokenizer  # ä¿ç•™ä»¥é˜² net_class éœ€è¦

# å¯¼å…¥é¡¹ç›®å†…éƒ¨çš„æ³¨å†Œå™¨ä¸å·¥å…·å‡½æ•°ï¼ˆä¸ dplm.py ä¿æŒä¸€è‡´ï¼‰
from byprot.models import register_model
from byprot.models.utils import (
    LoRAConfig,
    NetConfig,
    get_net,
    get_net_class,
    sample_from_categorical,
    stochastic_sample_from_categorical,
    top_k_top_p_filtering,
    topk_masking,
)
from byprot import utils

log = utils.get_logger(__name__)

# å°è¯•å¯¼å…¥ CrystaTokenizerWrapperï¼šä½ éœ€è¦æŠŠ tokenizer æ”¾åœ¨å·¥ç¨‹é‡Œä¸‹åˆ—è·¯å¾„ä¹‹ä¸€
# æ¨èä½ç½®ï¼š src/dmlm/tokenizers/crysta_tokenizer.py æˆ– é¡¹ç›®æ ¹ dmlm/tokenizers/crysta_tokenizer.py
try:
    # å…ˆå°è¯•å·¥ç¨‹å†…æ¨¡å—é£æ ¼å¯¼å…¥ï¼ˆå¦‚æœä½ æŠŠ tokenizer æ”¾åœ¨ dmlm/tokenizers ä¸‹ï¼‰
    from dmlm.tokenizers.crysta_tokenizer import CrystaTokenizerWrapper
except Exception:
    try:
        # å†å°è¯•æŒ‰ byprot åŒ…å†…å¯¼å…¥ï¼ˆå¦‚æœä½ æ”¾å…¥ byprot.tokenizersï¼‰
        from byprot.tokenizers.crysta_tokenizer import CrystaTokenizerWrapper
    except Exception:
        # å¦‚æœéƒ½ä¸èƒ½å¯¼å…¥ï¼Œç»™å‡ºå‹å¥½æç¤ºå¹¶åœ¨è¿è¡Œæ—¶æŠ›é”™
        CrystaTokenizerWrapper = None


# é»˜è®¤é…ç½® dataclassï¼ˆä¸ DPLMConfig ä¿æŒå­—æ®µä¸€è‡´ï¼Œæ–¹ä¾¿æ›¿æ¢é…ç½®ï¼‰
@dataclass
class DMLMConfig:
    # æ‰©æ•£æ—¶é—´æ­¥æ•°é‡ï¼ˆå’Œ DPLM ä¸€è‡´ï¼‰
    num_diffusion_timesteps: int = field(default=500)
    # LoRA é…ç½®ï¼ˆå¦‚æœä½¿ç”¨ LoRAï¼‰
    lora: LoRAConfig = field(default=LoRAConfig())
    # ç½‘ç»œé…ç½®ï¼ˆget_net ä¼šä½¿ç”¨ï¼‰
    net: NetConfig = field(default=NetConfig())
    # æ˜¯å¦å¯ç”¨ gradient checkpoint
    gradient_ckpt: bool = field(default=False)
    # æ˜¯å¦å¯ç”¨ rdm_couple è€¦åˆè®­ç»ƒ
    rdm_couple: bool = field(default=False)


# æ³¨å†Œæ¨¡å‹åä¸º "dmlm"ï¼ŒHydra/registry å¯ä»¥ç”¨è¿™ä¸ªåå­—å®ä¾‹åŒ–
@register_model("dmlm")
class DiffusionMaterialLanguageModel(nn.Module):
    """DMLMï¼šåŸºäº DPLM æ¶æ„çš„æ™¶ä½“è¯­è¨€æ¨¡å‹ã€‚

    è¯´æ˜ï¼š
    - backbone (self.net) ç”± get_net(self.cfg) åˆ›å»ºï¼ˆä¸ DPLM ä¿æŒä¸€è‡´ï¼‰ã€‚
    - tokenizer ä½¿ç”¨ CrystaTokenizerWrapperï¼ˆéœ€è¦åœ¨é¡¹ç›®ä¸­å‡†å¤‡å¥½ï¼‰ã€‚
    - å¦‚æœä»å¤´è®­ç»ƒï¼Œè¯·ç¡®ä¿ embedding å¤§å°ä¸ tokenizer.vocab_size å¯¹é½ï¼š
        - å¦‚æœ net å†…éƒ¨å·²æœ‰ embeddingï¼ˆæ¯”å¦‚ net.embed_tokens æˆ– net.lm_headï¼‰ï¼Œ
          ä»£ç ä¸­æä¾›äº† `maybe_resize_token_embeddings` çš„è°ƒç”¨ä½ç½®ç”¨äºé‡æ–°åˆå§‹åŒ– embeddingã€‚
    """

    _default_cfg = DMLMConfig()  # é»˜è®¤é…ç½®

    def __init__(self, cfg, net=None, from_dplm_weights=False, crysta_meta=None, num_diffusion_timesteps=None, **kwargs):
        """
        Args:
            cfg: OmegaConf / dict é…ç½®ï¼Œä¼šä¸ _default_cfg åˆå¹¶
            net: å¦‚æœå¤–éƒ¨ä¼ å…¥ net å®ä¾‹åˆ™ä½¿ç”¨ï¼Œå¦åˆ™ç”± get_net(self.cfg) åˆ›å»º
            from_dplm_weights: å¦‚æœ True å°è¯•åŠ è½½ dplm æƒé‡ï¼ˆé»˜è®¤ Falseï¼Œå› ä½ è¦ä»å¤´è®­ç»ƒï¼‰
            crysta_meta: å¯é€‰ï¼Œæ¥è‡ª meta.pkl çš„ dictï¼Œç”¨äºåˆå§‹åŒ– tokenizerï¼ˆå¦‚æœéœ€è¦ï¼‰
        """
        log.info(f'Function DiffusionMaterialLanguageModel.__init__() start.')
        self.num_diffusion_timesteps = num_diffusion_timesteps
        super().__init__()

        # åˆå¹¶å¹¶ä¿å­˜é…ç½®ï¼ˆå°†ç”¨æˆ· cfg ä¸é»˜è®¤ cfg åˆå¹¶ï¼‰
        self._update_cfg(cfg)
        log.info(f'Function DiffusionMaterialLanguageModel.__init__() Step 1: creating net.')
        # -------- 1) åˆå§‹åŒ– backbone/net ----------
        # å¦‚æœå¤–éƒ¨ä¼ å…¥ netï¼Œå°±ç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æ ¹æ® cfg åˆ›å»ºï¼ˆä¸ DPLM ä¸€è‡´ï¼‰
        self.net = get_net(self.cfg) if net is None else net

        log.info(f'Function DiffusionMaterialLanguageModel.__init__() Step 2: creating tokenizer.')
        # -------- 2) åˆå§‹åŒ–å¹¶æ›¿æ¢ tokenizer ----------
        # å¦‚æœ CrystaTokenizerWrapper æœªèƒ½ importï¼Œä¸Šé¢ CrystaTokenizerWrapper ä¼šä¸º None
        if CrystaTokenizerWrapper is None:
            raise ImportError(
                "CrystaTokenizerWrapper æœªæ‰¾åˆ°ã€‚è¯·å°† dmlm/tokenizers/crysta_tokenizer.py æ”¾åˆ°é¡¹ç›®ä¸­å¹¶ä¿è¯å¯ importã€‚"
            )
        else:
            log.info(f'Function DiffusionMaterialLanguageModel.__init__(): Import CrystaTokenizerWrapper success.')

        # åˆ›å»º crysta tokenizer å®ä¾‹ï¼›å¦‚æœ metaï¼ˆmeta.pklï¼‰å¯ç”¨ï¼Œä¼˜å…ˆä¼ å…¥
        try:
            # å°è¯•ä»¥ meta åˆå§‹åŒ–ï¼ˆå¦‚æœä½ ä¼ å…¥äº† crysta_metaï¼‰
            if crysta_meta is not None:
                self.tokenizer = CrystaTokenizerWrapper(meta=crysta_meta)
            else:
                self.tokenizer = CrystaTokenizerWrapper()
        except TypeError:
            # å¦‚æœå°è£…å™¨ä¸æ¥å— meta å‚æ•°ï¼Œé€€å›åˆ°æ— å‚æ„é€ 
            self.tokenizer = CrystaTokenizerWrapper()
        log.info(f'Function DiffusionMaterialLanguageModel.__init__(): Instancing CrystaTokenizerWrapper success.')

        # å°† tokenizer ç»‘å®šåˆ° net ä¸Šï¼ˆè¦†ç›– net çš„ tokenizerï¼Œä»¥ç¡®ä¿ä¸€è‡´ï¼‰
        # è¿™æ · net åœ¨ç”Ÿæˆ logits æ—¶å¦‚æœä¾èµ– self.net.tokenizer å¯ä»¥ä½¿ç”¨ CrystaTokenizerWrapper
        try:
            self.net.tokenizer = self.tokenizer
        except Exception:
            # å¦‚æœ net æ²¡æœ‰ tokenizer å±æ€§ï¼Œåˆ™å¿½ç•¥
            pass

        log.info(f'Function DiffusionMaterialLanguageModel.__init__() Step 3: special token id.')
        # -------- 3) ç‰¹æ®Š token idï¼ˆåŒ DPLMï¼‰ ----------
        # è¿™äº› id æœŸæœ›ä» net æˆ– tokenizer æä¾›
        # ä¼˜å…ˆä» net å–ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰ï¼Œå¦‚æœ net æ²¡æä¾›åˆ™ä» tokenizer å–
        self.mask_id = getattr(self.net, "mask_id", None) or getattr(self.tokenizer, "mask_token_id", None)
        self.pad_id = getattr(self.net, "pad_id", None) or getattr(self.tokenizer, "pad_token_id", None)
        self.bos_id = getattr(self.net, "bos_id", None) or getattr(self.tokenizer, "bos_token_id", None)
        self.eos_id = getattr(self.net, "eos_id", None) or getattr(self.tokenizer, "eos_token_id", None)
        #  self.x_id = getattr(self.net, "x_id", None)  # æœ‰äº›å®ç°ä¼šç”¨ x_id ä»£è¡¨ç‰¹æ®Šå ä½

        # -------- 4) å¦‚æœéœ€è¦ï¼Œä» dplm æƒé‡åŠ è½½ï¼ˆç”¨æˆ·è¦æ±‚ä»å¤´è®­ç»ƒæ—¶æ— éœ€å¯ç”¨ï¼‰ ----------
        if from_dplm_weights:
            # è¿™é‡Œä¿ç•™æ¥å£ï¼Œä½†é»˜è®¤ä¸ä½¿ç”¨ã€‚è‹¥å¯ç”¨ï¼Œget_net_class ç­‰ä»£ç ä¼šå°è¯•åŠ è½½å¯¹åº”æƒé‡ã€‚
            # å…·ä½“åŠ è½½é€»è¾‘å¯å‚è€ƒåŸ dplm.py çš„ from_pretrained å®ç°ï¼ˆä¸åœ¨æ­¤é‡å¤ï¼‰ã€‚
            pass

        log.info(f'Function DiffusionMaterialLanguageModel.__init__() Step 5: vocab size.')
        # -------- 5) è‹¥ net çš„ embedding ä¸ Crysta vocab ä¸ä¸€è‡´ -> é‡æ–°åˆå§‹åŒ– embedding ----------
        # å¾ˆå¤š net å®ç°ä¼šåŒ…å«ä¸€ä¸ª embedding å±‚ï¼Œä¾‹å¦‚ attribute åä¸º "embed_tokens" æˆ– "embeddings.weight"
        # æˆ‘ä»¬å°è¯•å°½å¯èƒ½å‘ç°å¹¶è°ƒæ•´ embedding çš„å¤§å°ä»¥åŒ¹é… tokenizer.vocab_size
        crysta_vocab_size = getattr(self.tokenizer, "vocab_size", None)
        if crysta_vocab_size is not None:
            # å°è¯•å‡ ç§å¸¸è§ embedding å±æ€§å
            # 1) Common HF style: net.get_input_embeddings() / net.resize_token_embeddings
            if hasattr(self.net, "resize_token_embeddings"):
                # å¦‚æœ net æ”¯æŒ resizeï¼ˆå¦‚ transformers-basedï¼‰ï¼Œè°ƒç”¨å®ƒ
                log.info(f'net æ”¯æŒ resize.')
                try:
                    self.net.resize_token_embeddings(crysta_vocab_size)
                    log.info(f'Resize token embedding Done.')
                except Exception:
                    # å¦‚æœå¤±è´¥ï¼Œä¸äº§ç”Ÿè‡´å‘½é”™è¯¯ï¼Œåªæ‰“å°æç¤º
                    log.error(f'resize_token_embeddings å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ net çš„ embedding å¹¶è°ƒæ•´ä¸º crysta_vocab_sizeã€‚')
            else:
                # 2) ç›´æ¥æŸ¥æ‰¾ embed_tokens æˆ– embeddings
                log.info(f'net ä¸æ”¯æŒ resize.')
                if hasattr(self.net, "embed_tokens"):
                    old = self.net.embed_tokens
                    if getattr(old, "num_embeddings", None) != crysta_vocab_size:
                        # ç›´æ¥æ›¿æ¢ä¸ºæ–°çš„ nn.Embedding å¹¶åˆå§‹åŒ–
                        hidden = old.embedding_dim if hasattr(old, "embedding_dim") else old.weight.size(1)
                        new_emb = nn.Embedding(crysta_vocab_size, hidden)
                        # ä½¿ç”¨ä¸åŸæ¥ç›¸åŒçš„åˆå§‹åŒ–æ–¹æ³•
                        nn.init.normal_(new_emb.weight, mean=0.0, std=0.02)
                        self.net.embed_tokens = new_emb
                        log.info(f"å·²å°† net.embed_tokens é‡ç½®ä¸ºå¤§å° {crysta_vocab_size} x {hidden}")
                elif hasattr(self.net, "embeddings") and hasattr(self.net.embeddings, "word_embeddings"):
                    # ESM é£æ ¼æˆ–å…¶ä»–å¯èƒ½ä½¿ç”¨ embeddings.word_embeddings
                    we = self.net.embeddings.word_embeddings
                    if getattr(we, "num_embeddings", None) != crysta_vocab_size:
                        hidden = we.embedding_dim if hasattr(we, "embedding_dim") else we.weight.size(1)
                        new_we = nn.Embedding(crysta_vocab_size, hidden)
                        nn.init.normal_(new_we.weight, mean=0.0, std=0.02)
                        self.net.embeddings.word_embeddings = new_we
                        log.info(f"å·²å°† net.embeddings.word_embeddings é‡ç½®ä¸ºå¤§å° {crysta_vocab_size} x {hidden}")
                else:
                    # æœªèƒ½è¯†åˆ« embedding ç»“æ„ï¼Œç»™å‡ºæç¤ºï¼Œç”¨æˆ·å¯æ‰‹åŠ¨è°ƒæ•´ net å®šä¹‰ä»¥åŒ¹é… vocab_size
                    log.warning("è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° net å¸¸è§ embedding å±æ€§"
                                "ï¼ˆresize_token_embeddings/embed_tokens/embeddings.word_embeddingsï¼‰ï¼Œ"
                                "è¯·æ‰‹åŠ¨ç¡®ä¿ embedding å¤§å°ä¸ tokenizer.vocab_size å¯¹é½ã€‚")

        # -------- 6) å¦‚æœé…ç½®å¼€å¯ gradient checkpointï¼Œåˆ™å¯ç”¨ net çš„ checkpointingï¼ˆä¸ DPLM ä¸€è‡´ï¼‰ ----------
        if self.cfg.gradient_ckpt:
            if hasattr(self.net, "supports_gradient_checkpointing"):
                self.net.supports_gradient_checkpointing = True
                try:
                    # ä¸€äº›æ¨¡å‹ API æ”¯æŒ gradient_checkpointing_enable()
                    self.net.gradient_checkpointing_enable()
                except Exception:
                    pass
        log.info(f'Function DiffusionMaterialLanguageModel.__init__() Done.')


    # ä¸ DPLM ä¿æŒä¸€è‡´çš„ from_pretrained æ¥å£ï¼ˆä¿ç•™ï¼Œä¾¿äºæœªæ¥åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰
    @classmethod
    def from_pretrained(
        cls, net_name, cfg_override={}, net_override={}, from_huggingface=False
    ):
        """
        å‚è€ƒ DPLM çš„ from_pretrained å®ç°ï¼šä¿ç•™æœ¬æ¥å£ä»¥ä¾¿å°†æ¥åŠ è½½ checkpointã€‚
        """
        from pathlib import Path
        from collections import OrderedDict
        import json
        import torch

        if not from_huggingface:
            # local checkpoint åŠ è½½ï¼ˆä¸åŸ dplm ç›¸åŒçš„é€»è¾‘ï¼‰
            from byprot.utils.config import load_yaml_config

            # cfg_path = Path(net_name).parents[1]
            # cfg_path = Path(cfg_path, ".hydra", "config.yaml")
            cfg_path = Path("configs", "config_all.yaml")
            # åŠ è½½å®Œæ•´é…ç½®
            full_cfg = load_yaml_config(str(cfg_path))
            print(f"Loaded config keys: {list(full_cfg.keys())}")  # è°ƒè¯•ä¿¡æ¯
            cfg = load_yaml_config(str(cfg_path)).model
            cfg.net.pretrain = False
            # å®‰å…¨åœ°ç§»é™¤ _target_
            if "_target_" in cfg:
                cfg.pop("_target_")
            model = cls(cfg)

            pretrained_state_dict = torch.load(
                net_name, map_location=torch.device("cpu")
            )["state_dict"]
            new_pretrained_state_dict = OrderedDict()

            # remove the "model." prefix if present
            for k, v in pretrained_state_dict.items():
                new_pretrained_state_dict[k[6:]] = v

            missing, unexpected = model.load_state_dict(
                new_pretrained_state_dict, strict=False
            )
            print(
                f"Restored from {net_name} with {len(missing)} missing and {len(unexpected)} unexpected keys"
            )
            if len(missing) > 0:
                print(f"Missing Keys: {missing}")
                print(f"Unexpected Keys: {unexpected}")
            return model
        else:
            # å¦‚æœéœ€è¦ä» HuggingFace æˆ–æœ¬åœ° HF mirror åŠ è½½ç½‘ç»œï¼ˆä¿ç•™æ¥å£ï¼‰
            # è¿™é‡Œç¤ºä¾‹ä½¿ç”¨ local_dir æ–¹å¼ï¼ˆå¦‚ dplm.py ä¸­ï¼‰ï¼Œä½ å¯ä»¥æŒ‰éœ€ä¿®æ”¹
            local_dir = "airkingbd/dmlm_650m"  # å¦‚æœå­˜åœ¨æœ¬åœ° HF é£æ ¼ä»“åº“å¯æ”¹ä¸ºä½ çš„è·¯å¾„
            if local_dir is None:
                raise ValueError(
                    "`local_dir` must be provided when `from_huggingface=True` and server cannot access HuggingFace."
                )

            config_path = Path(local_dir, "config.json")
            if not config_path.exists():
                raise FileNotFoundError(f"Config file not found at {config_path}")

            with open(config_path, "r") as f:
                config = json.load(f)
            dplm_type = config.get("dplm_type")  # ä¿æŒå­—æ®µåä»¥å…¼å®¹åŸæœ‰å®ç°ï¼ˆå¯èƒ½éœ€è¦æ”¹åï¼‰
            if dplm_type is None:
                raise ValueError("`dplm_type` not found in config.json")

            net_class = get_net_class(dplm_type)
            net = net_class.from_pretrained(str(local_dir), **net_override)

            return cls(cfg=cfg_override, net=net)

    # åˆå¹¶é…ç½®ï¼ˆä¸ DPLM çš„ _update_cfg å®Œå…¨ä¸€è‡´ï¼‰
    def _update_cfg(self, cfg):
        # # åŸæ¥çš„ä»£ç ï¼š
        # self.cfg = OmegaConf.merge(self._default_cfg, cfg)

        # ä¿®æ”¹ä¸ºï¼š
        try:
            self.cfg = OmegaConf.merge(self._default_cfg, cfg)
        except Exception as e:
            print(f"é…ç½®åˆå¹¶å¤±è´¥: {e}")
            print("ä½¿ç”¨æ–‡ä»¶é…ç½®ï¼Œå¿½ç•¥é»˜è®¤é…ç½®")
            self.cfg = cfg  # ç›´æ¥ä½¿ç”¨æ–‡ä»¶é…ç½®

    # ä»¥ä¸‹å‡½æ•°ï¼ˆq_sample_coupled / q_sample / forward / compute_loss / generate ç­‰ï¼‰
    # åŸºæœ¬ä¿ç•™ DPLM çš„åŸå§‹å®ç°ï¼Œé€è¡Œæ³¨é‡Šä»¥ä¾¿ç†è§£ã€‚
    # ---- q_sample_coupled ----
    def q_sample_coupled(self, x_0, t1, t2, maskable_mask):
        # t1_eq_t2_mask è¡¨ç¤ºå“ªäº›åºåˆ—çš„ä¸¤ä¸ªæ—¶é—´æ­¥ç›¸ç­‰ï¼ˆç”¨äºè€¦åˆç­–ç•¥ï¼‰
        t1_eq_t2_mask = t1 == t2
        # å°† t1, t2 è§„æ•´ä¸º t1>=t2
        t1, t2 = torch.maximum(t1, t2).float(), torch.minimum(t1, t2).float()

        # sample t1
        u = torch.rand_like(x_0, dtype=torch.float)
        # å¯¹æ¯ä¸ªä½ç½®ä»¥æ¦‚ç‡ï¼ˆt1/num_timestepsï¼‰å†³å®šæ˜¯å¦ maskï¼ˆå–ä»£ä¸º mask_idï¼‰
        t1_mask = (
            u < (t1 / self.cfg.num_diffusion_timesteps)[:, None]
        ) & maskable_mask
        # å°†é€‰ä¸­ä½ç½®æ›¿æ¢æˆ mask_idï¼Œå¾—åˆ° x_t1
        x_t1 = x_0.masked_fill(t1_mask, self.mask_id)

        # sample t2
        u = torch.rand_like(x_0, dtype=torch.float)
        # åœ¨å·²ç»è¢« t1_mask æ ‡è®°çš„ä½ç½®ï¼ŒæŒ‰æ¯”ä¾‹å†³å®šæ˜¯å¦ä¿ç•™åœ¨ t2
        t2_mask = t1_mask & (u > ((t1 - t2) / t1)[:, None])
        u = torch.rand_like(x_0[t1_eq_t2_mask], dtype=torch.float)
        # å¯¹äº t1==t2 çš„æƒ…å†µï¼ŒæŒ‰ç‰¹æ®Šè§„åˆ™å¤„ç†
        t2_mask[t1_eq_t2_mask] = (
            u < (t1[t1_eq_t2_mask] / self.cfg.num_diffusion_timesteps)[:, None]
        ) & (maskable_mask[t1_eq_t2_mask])
        x_t2 = x_0.masked_fill(t2_mask, self.mask_id)

        # è¿”å›æ‹¼æ¥åçš„ç»“æœï¼šx_t (ä¸¤ä¸ª batch ç»´åº¦æ‹¼æ¥)ï¼Œtï¼ˆæ—¶é—´æ­¥ï¼‰ï¼Œä»¥åŠ mask æ©ç 
        return {
            "x_t": torch.cat([x_t1, x_t2], dim=0),
            "t": torch.cat([t1, t2]),
            "mask_mask": torch.cat([t1_mask, t2_mask], dim=0),
        }

    # ---- q_sample ----
    def q_sample(self, x_0, t1, maskable_mask):
        # sample t1
        u = torch.rand_like(x_0, dtype=torch.float)
        t1_mask = (
            u < (t1 / self.cfg.num_diffusion_timesteps)[:, None]
        ) & maskable_mask
        x_t1 = x_0.masked_fill(t1_mask, self.mask_id)
        # æ³¨æ„ï¼šåŸ dplm é‡Œæœ‰ä¸¤æ¬¡ masked_fillï¼ˆå¯èƒ½æ˜¯ç¬”è¯¯æˆ–å†—ä½™ï¼‰ï¼Œä¿ç•™ä»¥å…¼å®¹
        x_t1 = x_t1.masked_fill(t1_mask, self.mask_id)

        return {
            "x_t": x_t1,
            "t": t1,
            "mask_mask": t1_mask,
        }

    # ---- forward: ä½¿ç”¨ net äº§ç”Ÿ logits ----
    def forward(self, input_ids, return_last_hidden_state=False, **kwargs):
        # net çš„æ¥å£ä¸ DPLM ä¿æŒä¸€è‡´ï¼šä¼ å…¥ input_idsï¼Œè¿”å› dictï¼ŒåŒ…å« "logits" å’Œå¯é€‰çš„ "last_hidden_state"
        outputs = self.net(
            input_ids=input_ids,
        )
        logits = outputs["logits"]
        if return_last_hidden_state:
            last_hidden_state = outputs["last_hidden_state"]
            return logits, last_hidden_state
        else:
            return logits

    # ---- compute_loss: è®­ç»ƒæ—¶ç”¨çš„é‡‡æ · + æŸå¤±è®¡ç®—é€»è¾‘ ----
    def compute_loss(self, batch, weighting="constant"):
        # batch é‡ŒæœŸæœ›å«æœ‰ "targets"ï¼ˆå³ ground truth token idsï¼‰
        """
        # print("=" * 80)
        # print("ğŸŸ¢ COMPUTE_LOSS FUNCTION START")
        # print(f"ğŸ” BATCH ç±»å‹: {type(batch)}")
        # #  <class 'dict'>
        # print(f"ğŸ” BATCH å†…å®¹:")
          Key: 'input_ids'
            Type: <class 'torch.Tensor'>
            Shape: torch.Size([2, 2048])
            Dtype: torch.int64
            Device: cuda:0
            Values[0, :5]: [142, 142, 124, 2, 91]
          Key: 'targets'
            Type: <class 'torch.Tensor'>
            Shape: torch.Size([2, 2048])
            Dtype: torch.int64
            Device: cuda:0
            Values[0, :5]: [142, 142, 124, 2, 91]
          Key: 'input_mask'
            Type: <class 'torch.Tensor'>
            Shape: torch.Size([2, 2048])
            Dtype: torch.bool
            Device: cuda:0
            Values[0, :5]: [True, True, True, True, True]
        """
        # print(type(batch), batch.keys() if isinstance(batch, dict) else batch.shape)
        target = batch["targets"]
        batch_size = target.size(0)
        """
        print("\nğŸŸ¢ STEP 2: é‡‡æ ·æ—¶é—´æ­¥ t1, t2")
        print(f"  Batch size: {batch_size}")
        print(f"  num_diffusion_timesteps: {self.cfg.num_diffusion_timesteps}")
        Batch size: 2
        num_diffusion_timesteps: 500
        """
        # éšæœºé‡‡æ ·ä¸¤ä¸ªæ—¶é—´æ­¥ t1, t2ï¼ˆé•¿åº¦ä¸º 2*Bï¼Œéšå chunk æˆä¸¤ä¸ªå‘é‡ï¼‰
        t1, t2 = torch.randint(
            1,
            self.cfg.num_diffusion_timesteps + 1,
            (2 * target.size(0),),
            device=target.device,
        ).chunk(2)
        """
        print(f"  t1 shape: {t1.shape}")
        print(f"  t2 shape: {t2.shape}")
        print(f"  t1 å€¼: {t1.cpu().tolist()}")
        print(f"  t2 å€¼: {t2.cpu().tolist()}")
          t1 shape: torch.Size([2])
          t2 shape: torch.Size([2])
          t1 å€¼: [144, 410]
          t2 å€¼: [299, 251]
        """
        # å¦‚æœå¯ç”¨ rdm_coupleï¼Œåˆ™ä½¿ç”¨è€¦åˆæ ·æœ¬ç­–ç•¥ï¼ˆä¸è®ºæ–‡/å®ç°å¯¹åº”ï¼‰
        if self.cfg.rdm_couple:
            print("  ğŸ”„ ä½¿ç”¨ q_sample_coupled ç­–ç•¥")
            x_t, t, loss_mask = list(
                self.q_sample_coupled(
                    target,
                    t1,
                    t2,
                    maskable_mask=self.get_non_special_symbol_mask(target),
                ).values()
            )
            print(f"    x_t shape: {x_t.shape}")
            print(f"    t shape: {t.shape}")
            print(f"    loss_mask shape: {loss_mask.shape}")
            # ç›®æ ‡ä¹Ÿéœ€è¦é‡å¤ä¸€æ¬¡ä»¥åŒ¹é… x_t çš„ batch ç»´åº¦ï¼ˆå› ä¸ºè€¦åˆæŠŠ batch ç¿»å€ï¼‰
            target = target.repeat(2, 1)
        else:
            # å¦åˆ™ä½¿ç”¨æ™®é€š q_sample
            # print("  ğŸ”„ ä½¿ç”¨æ™®é€š q_sample ç­–ç•¥")
            x_t, t, loss_mask = list(
                self.q_sample(
                    target,
                    t1,
                    maskable_mask=self.get_non_special_symbol_mask(target),
                ).values()
            )
            """
            print(f"    x_t shape: {x_t.shape}")
            print(f"    t shape: {t.shape}")
            print(f"    loss_mask shape: {loss_mask.shape}")
            x_t shape: torch.Size([2, 2048])
            t shape: torch.Size([2])
            loss_mask shape: torch.Size([2, 2048])
            """
        """
        print("\nğŸŸ¢ STEP 5: æ‰©æ•£è¿‡ç¨‹è¾“å‡ºè¯¦ç»†æ£€æŸ¥")
        print(f"  x_t (æ·»åŠ å™ªå£°åçš„token):")
        print(f"    shape: {x_t.shape}")
        print(f"    dtype: {x_t.dtype}")
        print(f"    ç¤ºä¾‹å€¼ (batch=0, first 10): {x_t[0, :10].cpu().tolist()}")
        print(f"    å”¯ä¸€å€¼: {torch.unique(x_t).cpu().tolist()}")

        print(f"  t (æ—¶é—´æ­¥):")
        print(f"    shape: {t.shape}")
        print(f"    dtype: {t.dtype}")
        print(f"    å€¼: {t.cpu().tolist()}")

        print(f"  loss_mask (æŸå¤±mask):")
        print(f"    shape: {loss_mask.shape}")
        print(f"    dtype: {loss_mask.dtype}")
        print(f"    Trueçš„æ•°é‡: {loss_mask.sum().item()}")
        print(f"    æ¯”ä¾‹: {loss_mask.sum().item() / loss_mask.numel():.3f}")
        print(f"    ç¤ºä¾‹ (batch=0, first 10): {loss_mask[0, :10].cpu().tolist()}")
        
        ğŸŸ¢ STEP 5: æ‰©æ•£è¿‡ç¨‹è¾“å‡ºè¯¦ç»†æ£€æŸ¥
          x_t (æ·»åŠ å™ªå£°åçš„token):
            shape: torch.Size([2, 2048])
            dtype: torch.int64
            ç¤ºä¾‹å€¼ (batch=0, first 10): [142, 374, 124, 2, 91, 11, 93, 142, 123, 142]
            å”¯ä¸€å€¼: [0, 2, 11, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 102, 107, 116, 123, 124, 125, 126, 127, 128, 133, 141, 142, 265, 374]
          t (æ—¶é—´æ­¥):
            shape: torch.Size([2])
            dtype: torch.int64
            å€¼: [144, 410]
          loss_mask (æŸå¤±mask):
            shape: torch.Size([2, 2048])
            dtype: torch.bool
            Trueçš„æ•°é‡: 2320
            æ¯”ä¾‹: 0.566
            ç¤ºä¾‹ (batch=0, first 10): [False, True, False, False, False, False, False, False, False, False]
        """
        # forward å¾—åˆ° logits
        logits = self.forward(x_t)

        """
        

        print(f"  âœ… æ­£å‘ä¼ æ’­å®Œæˆ")
        print(f"  logits shape: {logits.shape}")
        print(f"  logits dtype: {logits.dtype}")
        print(f"  logits device: {logits.device}")

        # æ£€æŸ¥logitsçš„æœ‰æ•ˆæ€§
        if torch.isnan(logits).any():
            print("  âš ï¸ è­¦å‘Š: logits åŒ…å« NaN å€¼!")
            print(f"    NaN æ•°é‡: {torch.isnan(logits).sum().item()}")

        if torch.isinf(logits).any():
            print("  âš ï¸ è­¦å‘Š: logits åŒ…å« Inf å€¼!")
            print(f"    Inf æ•°é‡: {torch.isinf(logits).sum().item()}")

        # æ˜¾ç¤ºlogitsçš„ç»Ÿè®¡ä¿¡æ¯
        print(f"  logits ç»Ÿè®¡:")
        print(f"    min: {logits.min().item():.6f}")
        print(f"    max: {logits.max().item():.6f}")
        print(f"    mean: {logits.mean().item():.6f}")
        print(f"    std: {logits.std().item():.6f}")
        
          âœ… æ­£å‘ä¼ æ’­å®Œæˆ
              logits shape: torch.Size([2, 2048, 375])
              logits dtype: torch.float32
              logits device: cuda:0
              logits ç»Ÿè®¡:
                min: -3.312500
                max: 3.062500
                mean: -0.008304
                std: 0.738563
        """
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„æƒé‡ï¼ˆlinear æˆ– constantï¼‰
        num_timesteps = self.cfg.num_diffusion_timesteps
        weight = {
            "linear": (
                num_timesteps - (t - 1)
            ),  # num_timesteps * (1 - (t-1)/num_timesteps)
            "constant": num_timesteps * torch.ones_like(t),
        }[weighting][:, None].float() / num_timesteps
        """
        print(f"  æƒé‡è®¡ç®—å®Œæˆ:")
        print(f"    weight shape: {weight.shape}")
        print(f"    weight dtype: {weight.dtype}")
        print(f"    weight å€¼: {weight.squeeze().cpu().tolist()}")

        print(f"  è¿”å›çš„å…ƒç»„åŒ…å«:")
        print(f"    1. logits: shape={logits.shape}")
        print(f"    2. target: shape={target.shape}")
        print(f"    3. loss_mask: shape={loss_mask.shape}")
        print(f"    4. weight: shape={weight.shape}")
          æƒé‡è®¡ç®—å®Œæˆ:
            weight shape: torch.Size([2, 1])
            weight dtype: torch.float32
            weight å€¼: [0.7140000462532043, 0.18200001120567322]
          è¿”å›çš„å…ƒç»„åŒ…å«:
            1. logits: shape=torch.Size([2, 2048, 375])
            2. target: shape=torch.Size([2, 2048])
            3. loss_mask: shape=torch.Size([2, 2048])
            4. weight: shape=torch.Size([2, 1])
        """
        # è¿”å› logits, target, loss_mask å’Œæƒé‡ï¼ˆåç»­è®­ç»ƒ loop é‡Œä¼šç”¨è¿™äº›æ¥è®¡ç®— lossï¼‰
        return logits, target, loss_mask, weight

    # ---- forward_encoder: ç•™ç©ºï¼ŒæŒ‰éœ€æ‰©å±• ----
    def forward_encoder(self, input_tokens, **kwargs):
        # å¦‚æœéœ€è¦ encoder-conditional generationï¼Œå¯ä»¥åœ¨å­ç±»è¦†ç›–æ­¤æ–¹æ³•
        return {}

    # ---- initialize_output_tokens: ç”Ÿæˆåˆå§‹åŒ–çš„ output tokensï¼ˆç”¨ mask_id å¡«å……éœ€è¦é¢„æµ‹çš„ä½ç½®ï¼‰ ----
    def initialize_output_tokens(self, input_tokens, partial_masks=None, **kwargs):
        tokens = input_tokens
        if tokens is None:
            raise NotImplementedError
        else:
            # å¾—åˆ°å¯ä»¥è¢«é¢„æµ‹çš„ä½ç½®æ©ç ï¼ˆéç‰¹æ®Šç¬¦å·ï¼‰
            output_mask = self.get_non_special_symbol_mask(tokens, partial_masks=partial_masks)

            # å°†è¿™äº›ä½ç½®æ›¿æ¢ä¸º mask_idï¼Œä½œä¸ºåˆå§‹åŒ–
            output_tokens = tokens.masked_fill(output_mask, self.mask_id)
            # åˆå§‹åŒ–åˆ†æ•°å…¨ä¸º 0
            output_scores = torch.zeros_like(output_tokens, dtype=torch.float)

            return output_tokens, output_scores

    # ---- resample: ç”¨äº rejection sampling å»é™¤é‡å¤ token æ¨¡å¼ ----
    def resample(self, _tokens, _scores, ratio, scale):
        """Rejection sampling to reduce repetitive tokens (e.g., 'VVVVV...')"""

        to_be_resample_idx = []
        resample_input = []
        resample_input_mask = []
        resample_input_scores = []

        # ç»Ÿè®¡æ¯ä¸ªåºåˆ—é‡Œå„ token çš„å‡ºç°ä½ç½®ï¼Œæ‰¾å‡ºé¢‘ç‡æœ€é«˜çš„ token
        for i, seq in enumerate(_tokens):
            most_token_dict = {}
            most_token_num = -1
            for j, token in enumerate(seq):
                token = int(token)
                if token not in most_token_dict:
                    most_token_dict[token] = [j]
                else:
                    most_token_dict[token].append(j)
                if len(most_token_dict[token]) > most_token_num:
                    most_token_num = len(most_token_dict[token])
            # å¦‚æœæŸä¸ª token å‡ºç°æ¬¡æ•°è¶…è¿‡é˜ˆå€¼ï¼ˆlen(seq) * ratioï¼‰ï¼Œåˆ™æŠŠè¿™äº›ä½ç½®æ ‡ä¸ºéœ€è¦é‡é‡‡æ ·
            if most_token_num > len(seq) * ratio:
                to_be_resample_idx.append(i)
                resample_input_scores.append(_scores[i])
                mask = torch.zeros_like(seq).bool()
                for k, v in most_token_dict.items():
                    if len(v) > len(seq) * ratio:
                        mask |= seq.eq(k)
                resample_input_mask.append(mask)
                resample_input.append(seq.masked_fill(mask, self.mask_id))

        # å¦‚æœå­˜åœ¨éœ€è¦é‡é‡‡æ ·çš„åºåˆ—
        if len(to_be_resample_idx) > 0:
            # æŠŠè¦é‡é‡‡æ ·çš„åºåˆ—å †æˆ batch å¹¶è½¬å›ç›¸åŒ dtype
            resample_input = torch.stack(resample_input, dim=0).type_as(
                _tokens
            )
            resample_input_scores = torch.stack(
                resample_input_scores, dim=0
            ).type_as(_scores)
            resample_input_mask = (
                torch.stack(resample_input_mask, dim=0).type_as(_tokens).bool()
            )
            # é€šè¿‡ net é‡æ–°é¢„æµ‹ logits
            resample_logits = self.net(
                input_ids=resample_input,
            )["logits"]
            # ä¿è¯ç±»å‹ä¸€è‡´
            if resample_logits.dtype != _scores.dtype:
                resample_logits = resample_logits.type_as(_scores)
            # æŠŠç‰¹æ®Š token çš„ logits è®¾ä¸º -infï¼Œé¿å…è¢«é‡‡æ ·
            resample_logits[..., self.mask_id] = -math.inf
#            resample_logits[..., self.x_id] = -math.inf
            resample_logits[..., self.pad_id] = -math.inf
            resample_logits[..., self.bos_id] = -math.inf
            resample_logits[..., self.eos_id] = -math.inf

            # ä½¿ç”¨ top-k/top-p è¿‡æ»¤
            resample_logits = top_k_top_p_filtering(
                resample_logits, top_p=0.95
            )
            noise_scale = scale
            assert resample_logits.size(0) == len(to_be_resample_idx)
            (
                resample_tokens,
                resample_scores,
            ) = stochastic_sample_from_categorical(
                resample_logits, temperature=0.0, noise_scale=noise_scale
            )
            # æŠŠé‡é‡‡æ ·ç»“æœå†™å›åŸå§‹ä½ç½®
            resample_input.masked_scatter_(
                resample_input_mask, resample_tokens[resample_input_mask]
            )
            resample_input_scores.masked_scatter_(
                resample_input_mask, resample_scores[resample_input_mask]
            )
            _tokens[to_be_resample_idx], _scores[to_be_resample_idx] = (
                resample_input,
                resample_input_scores,
            )

    # ---- forward_decoder: decoder æ­¥éª¤ï¼ˆç”¨äºç”Ÿæˆï¼‰ ----
    def forward_decoder(
        self,
        prev_decoder_out,
        encoder_out=None,
        need_attn_weights=False,
        partial_masks=None,
        sampling_strategy="gumbel_argmax",
        disable_resample=True,
        resample_ratio=0.25,
    ):
        # æ‹·è´è¾“å…¥çŠ¶æ€ï¼Œé¿å…åŸåœ°ä¿®æ”¹å½±å“è°ƒç”¨æ–¹
        output_tokens = prev_decoder_out["output_tokens"].clone()
        output_scores = prev_decoder_out["output_scores"].clone()
        step, max_step = prev_decoder_out["step"], prev_decoder_out["max_step"]
        temperature = prev_decoder_out["temperature"]
        history = prev_decoder_out["history"]

        # è®¡ç®—å½“å‰å¯ä»¥é¢„æµ‹çš„ä½ç½®ï¼ˆéç‰¹æ®Šç¬¦å·ï¼‰
        output_masks = self.get_non_special_symbol_mask(
            output_tokens, partial_masks=partial_masks
        )

        # è°ƒç”¨ net å¾—åˆ° logitsï¼ˆæ¨¡å‹çš„ä¸»æ¥å£ï¼‰
        net_out = self.net(
            input_ids=output_tokens,
        )

        logits = net_out["logits"]
        attentions = net_out["attentions"] if need_attn_weights else None

        # ç±»å‹å¯¹é½ï¼šç¡®ä¿ logits ä¸ output_scores dtype ä¸€è‡´ï¼Œæ–¹ä¾¿åç»­æ¯”è¾ƒ/æ’åº
        if logits.dtype != output_scores.dtype:
            logits = logits.type_as(output_scores)

        # å°†ç‰¹æ®Š token çš„ logits è®¾ä¸º -infï¼Œé¿å…æ¨¡å‹ç”Ÿæˆå®ƒä»¬
        logits[..., self.mask_id] = -math.inf
#        logits[..., self.x_id] = -math.inf
        logits[..., self.pad_id] = -math.inf
        logits[..., self.bos_id] = -math.inf
        logits[..., self.eos_id] = -math.inf

        # æ ¹æ®ä¸åŒçš„é‡‡æ ·ç­–ç•¥é€‰æ‹© token
        if sampling_strategy == "vanilla":
            _tokens, _scores = sample_from_categorical(
                logits, temperature=temperature
            )
        elif sampling_strategy == "argmax":
            # ç›´æ¥å–æœ€å¤§æ¦‚ç‡
            _scores, _tokens = logits.max(-1)
        elif sampling_strategy == "gumbel_argmax":
            # ä½¿ç”¨ Gumbel + argmax çš„è¿‘ä¼¼éšæœºåŒ–é‡‡æ ·
            noise_scale = 1.0
            _tokens, _scores = stochastic_sample_from_categorical(
                logits, temperature=0.0, noise_scale=noise_scale
            )

            if not disable_resample:
                # è‹¥å…è®¸é‡é‡‡æ ·ï¼Œåˆ™è°ƒç”¨ rejection sampling æ¶ˆé™¤é‡å¤æ¨¡å¼
                self.resample(
                    _tokens, _scores, ratio=resample_ratio, scale=1.0
                )
        else:
            raise NotImplementedError

        # ä»…æŠŠé¢„æµ‹ä½ç½®å¡«å›å»ï¼ˆmasked_scatter_ ä¿è¯åªæ›¿æ¢ output_masks ä½ç½®ï¼‰
        output_tokens.masked_scatter_(output_masks, _tokens[output_masks])
        output_scores.masked_scatter_(output_masks, _scores[output_masks])

        # ä¿å­˜å†å²
        history.append(output_tokens.clone())

        return dict(
            output_tokens=output_tokens,
            output_scores=output_scores,
            attentions=attentions,  # å¯èƒ½åŒ…å«æ³¨æ„åŠ›æƒé‡
            step=step + 1,
            max_step=max_step,
            history=history,
            hidden_states=net_out.get("last_hidden_state", None),
        )

    # ---- get_non_special_symbol_mask: è®¡ç®—éç‰¹æ®Š token æ©ç  ----
    def get_non_special_symbol_mask(self, output_tokens, partial_masks=None):
        non_special_sym_mask = (
            output_tokens.ne(self.pad_id)
            & output_tokens.ne(self.bos_id)
            & output_tokens.ne(self.eos_id)
        )
        if partial_masks is not None:
            non_special_sym_mask &= ~partial_masks
        return non_special_sym_mask

    # ---- _reparam_decoding: reparam è§£ç ç­–ç•¥ï¼ˆå¤æ‚çš„ top-k / stochastic ç­–ç•¥å®ç°ï¼‰ ----
    def _reparam_decoding(
        self,
        output_tokens,
        output_scores,
        cur_tokens,
        cur_scores,
        decoding_strategy,
        xt_neq_x0,
        non_special_sym_mask,
        t,
        max_step,
        noise,
    ):
        """This function is used to perform reparameterized decoding."""
        # decoding_strategy æ ¼å¼: "reparam-<conditioning>-<topk_mode>-<schedule>"
        _, condition, topk_mode, schedule = decoding_strategy.split("-")

        # æ ¹æ® schedule è®¡ç®—å»å™ªç‡ rate
        if schedule == "linear":
            rate = 1 - t / max_step
        elif schedule == "cosine":
            rate = np.cos(t / max_step * np.pi * 0.5)
        else:
            raise NotImplementedError

        # è®¡ç®—ç”¨äº top-k çš„ cutoff é•¿åº¦ = éç‰¹æ®Š token æ•° * rate
        cutoff_len = (
            non_special_sym_mask.sum(1, keepdim=True).type_as(output_scores)
            * rate
        ).long()
        # å°†ç‰¹æ®Š token çš„åˆ†æ•°è®¾ä¸ºè¾ƒå¤§å€¼ï¼Œé¿å…è¢«é€‰ä¸­
        _scores_for_topk = cur_scores.masked_fill(
            ~non_special_sym_mask, 1000.0
        )

        # top-k çš„ä¸¤ç§æ¨¡å¼ï¼šstochastic (å¸¦ Gumbel å™ªå£°) æˆ– deterministic
        if topk_mode.startswith("stochastic"):
            noise_scale = float(topk_mode.replace("stochastic", ""))
            lowest_k_mask = topk_masking(
                _scores_for_topk,
                cutoff_len,
                stochastic=True,
                temp=noise_scale * rate,
            )
        elif topk_mode == "deterministic":
            lowest_k_mask = topk_masking(
                _scores_for_topk, cutoff_len, stochastic=False
            )
        else:
            raise NotImplementedError

        # ä¾æ® conditionï¼ˆcond/uncondï¼‰è®¡ç®— not_v1_tï¼Œä¸ top-k ç­–ç•¥ç›¸å…³
        if condition == "cond":
            not_v1_t = (
                (cur_tokens == output_tokens)
                & (cur_scores < output_scores)
                & lowest_k_mask
            )
        elif condition == "uncond":
            not_v1_t = lowest_k_mask
        else:
            raise NotImplementedError

        # å¯¹ b_t = 0 çš„ä½ç½®åšå¤„ç†ï¼ˆè‹¥åœ¨ lowest_k ä¸­åˆ™ç½®ä¸ºå™ªå£°ï¼‰
        not_v2_t = lowest_k_mask

        last_mask_position = xt_neq_x0
        masked_to_noise = (~xt_neq_x0 & not_v1_t) | (xt_neq_x0 & not_v2_t)
        # å°†è¢« mask_to_noise çš„ä½ç½®èµ‹å€¼ä¸º noiseï¼ˆtensor æˆ– scalarï¼‰
        if isinstance(noise, torch.Tensor):
            output_tokens.masked_scatter_(
                masked_to_noise, noise[masked_to_noise]
            )
        elif isinstance(noise, (int, float)):
            output_tokens.masked_fill_(masked_to_noise, noise)
        else:
            raise NotImplementedError(
                "noise should be either a tensor or a scalar"
            )
        # æŠŠå¯¹åº”ä½ç½®çš„åˆ†æ•°è®¾ç½®ä¸º -inf
        output_scores.masked_fill_(masked_to_noise, -math.inf)

        # masked_to_x0 è¡¨ç¤ºç½®ä¸ºå½“å‰ cur_tokens çš„ä½ç½®
        masked_to_x0 = xt_neq_x0 & ~not_v2_t
        output_tokens.masked_scatter_(masked_to_x0, cur_tokens[masked_to_x0])
        output_scores.masked_scatter_(masked_to_x0, cur_scores[masked_to_x0])
        assert ((masked_to_x0 & last_mask_position) == masked_to_x0).all()

        # è®¡ç®—å¹¶è¿”å›ä¸‹ä¸€ä¸ª not_b_tï¼ˆä¸ºä¸‹ä¸€æ­¥ä¿å­˜ï¼‰
        new_xt_neq_x0 = (xt_neq_x0 | not_v1_t) & not_v2_t
        assert (new_xt_neq_x0 == not_v2_t).all()
        return new_xt_neq_x0, output_tokens, output_scores

    # ---- generate: é«˜å±‚ç”Ÿæˆå¾ªç¯ï¼ˆè°ƒç”¨åˆå§‹åŒ–ã€decoder stepã€reparam ç­–ç•¥ï¼‰ ----
    def generate(
        self,
        input_tokens,
        tokenizer=None,
        max_iter=None,
        temperature=None,
        partial_masks=None,
        sampling_strategy="gumbel_argmax",
        disable_resample=False,
        resample_ratio=0.25,
    ):
        # ä¿æŒæ¥å£é£æ ¼ï¼šä¼ å…¥ tokenizer / max_iter / temperature ç­‰ï¼Œé»˜è®¤è¡Œä¸ºä¸ DPLM ä¸€è‡´
        tokenizer = tokenizer
        max_iter = max_iter
        temperature = temperature

        # 0) encoderï¼ˆå¯é€‰ï¼‰
        encoder_out = self.forward_encoder(input_tokens)
        # 1) åˆå§‹åŒ– output tokensï¼ˆç”¨ mask å¡«å……éœ€è¦é¢„æµ‹çš„ä½ç½®ï¼‰
        (
            initial_output_tokens,
            initial_output_scores,
        ) = self.initialize_output_tokens(
            input_tokens, encoder_out=encoder_out, partial_masks=partial_masks
        )
        prev_decoder_out = dict(
            output_tokens=initial_output_tokens,
            output_scores=initial_output_scores,
            output_masks=None,
            attentions=None,
            step=0,
            max_step=max_iter,
            history=[initial_output_tokens.clone()],
            temperature=temperature,
        )

        # è®¡ç®—åˆå§‹çš„ output_masks
        prev_decoder_out["output_masks"] = self.get_non_special_symbol_mask(
            prev_decoder_out["output_tokens"], partial_masks=partial_masks
        )

        # è¿­ä»£ decoding æ­¥éª¤
        # for step in tqdm(range(max_iter), desc="Decoding"):
        for step in range(max_iter):

            # 2.1: predict
            with torch.no_grad():
                decoder_out = self.forward_decoder(
                    prev_decoder_out=prev_decoder_out,
                    encoder_out=encoder_out,
                    partial_masks=partial_masks,
                    sampling_strategy=sampling_strategy,
                    disable_resample=disable_resample,
                    resample_ratio=resample_ratio,
                )

            output_tokens = decoder_out["output_tokens"]
            output_scores = decoder_out["output_scores"]

            # 2.2: å¯¹ä½ç½®ä¿¡åº¦éƒ¨åˆ†é‡æ–°æ©ç å¹¶ä½¿ç”¨ reparam è§£ç ç­–ç•¥
            non_special_sym_mask = self.get_non_special_symbol_mask(
                prev_decoder_out["output_tokens"], partial_masks=partial_masks
            )

            (
                output_masks,
                result_tokens,
                result_scores,
            ) = self._reparam_decoding(
                output_tokens=prev_decoder_out["output_tokens"].clone(),
                output_scores=prev_decoder_out["output_scores"].clone(),
                cur_tokens=output_tokens.clone(),
                cur_scores=output_scores.clone(),
                decoding_strategy="reparam-uncond-deterministic-linear",
                xt_neq_x0=prev_decoder_out["output_masks"],
                non_special_sym_mask=non_special_sym_mask,
                t=step + 1,
                max_step=max_iter,
                noise=self.mask_id,
            )

            prev_decoder_out.update(output_masks=output_masks)
            output_tokens = result_tokens
            output_scores = result_scores

            prev_decoder_out.update(
                output_tokens=output_tokens,
                output_scores=output_scores,
                step=step + 1,
                history=decoder_out["history"],
            )

        decoder_out = prev_decoder_out
        # è¿”å›æœ€ç»ˆç”Ÿæˆ token çŸ©é˜µ
        return decoder_out["output_tokens"]
